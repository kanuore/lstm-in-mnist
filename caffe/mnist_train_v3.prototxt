# Enter your network definition here.
# Use Shift+Enter to update the visualization.
# Enter your network definition here.
# Use Shift+Enter to update the visualization.
name: "LeNet"
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/media/lbk/ubuntu file/dataset/mnist/caffe/mnist_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}

layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/media/lbk/ubuntu file/dataset/mnist/caffe/mnist_test_lmdb"
    batch_size: 128
    backend: LMDB
  }
}

# 将data[N,C,W,H]转换为 [T,Batch, H, C]
layer {
    name: "trans_data"
    type: "TensorTranspose"
    bottom: "data"
    top: "trans_data"
    tensor_transpose_param { 
      order: 2 
      order: 0 
      order: 3 
      order: 1
    }
}
# 这里把通道维度给剔除了，
# 但是之前没有加这一层也能训练并且效果还可以所以，
# 即使不剔除，以[H,C]作为输入也没关系，反正是全链接
layer {
  name: "reshape_data"
  type: "Reshape"
  bottom: "trans_data"
  top: "reshape_data"
  reshape_param {
    shape {
      dim:28 
      dim:-1
      dim:28
    }
  }
}

# 提供连续标志，证明连续一张像素行
layer {
  name: "cont"
  type: "HDF5Data"
  top: "cont"
  hdf5_data_param {
    source: "/home/lbk/ocr/LSTM/caffe/lstm_conv_h5.txt" 
    batch_size: 128
  }
}
#将 cont 转换为 [T, batch]
layer {
    name: "trans_cont"
    type: "TensorTranspose"
    bottom: "cont"
    top: "trans_cont"
    tensor_transpose_param { 
      order: 1 
      order: 0 
    }
}

layer {
  name: "lstm"
  type: "LSTM"
  bottom: "reshape_data"
  bottom: "trans_cont"
  top: "lstm"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  
  recurrent_param {
    num_output: 128 # 这里果然可以随便设置的，应该是参数w的大小
    weight_filler {
      type: "uniform"
      min: -0.08
      max: 0.08
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
#输出为28个时间步的 output [T, batch, 10] 只使用第28个时间步的值 output2
layer {
  name: "slice"
  type: "Slice"
  bottom: "lstm"
  top: "output1"
  top: "output2"
  slice_param {
    axis: 0
    slice_point: 27
  }
}

#output1 一直输出影响观察，这里随便加的一层，没有作用
layer {
  name: "reduction"
  type: "Reduction"
  bottom: "output1"
  top: "output11"
  reduction_param {
    axis: 0
  }
}

#将 cont 转换为 [batch,T]
layer {
    name: "trans_out"
    type: "TensorTranspose"
    bottom: "output2"
    top: "trans_out"
    tensor_transpose_param { 
      order: 1
      order: 0
      order: 2
    }
}

layer {
  name: "reshape_out"
  type: "Reshape"
  bottom: "trans_out"
  top: "reshape_out"
  reshape_param {
    shape {
      dim:-1 
      dim:1
      dim:1
      dim:128
    }
  }
}

layer {
  name: "fc"
  type: "InnerProduct"
  bottom: "reshape_out"
  top: "fc"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}

layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc"
  bottom: "label"
  top: "loss"
}

layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc"
  bottom: "label"
  top: "accuracy"
}
